{
 "cells": [
  {
   "cell_type": "raw",
   "id": "1ebcc487-a20e-4939-bf9e-4e68fb85c073",
   "metadata": {},
   "source": [
    "方式 1：只用 “report 本身”做质量评分维度如：Professionalism / Structure / Clarity / Actionability / Non-redundancy\n",
    "优点：不需要读年报 PDF，快、稳。\n",
    "缺点：不能严格衡量是否“忠实原文”。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "803b0eef-3735-4a1c-ae25-bc814e105c36",
   "metadata": {},
   "source": [
    "先写“批量读 report + 组织 prompt + 写出结果文件”（"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e284bbf5-25a4-49b7-bbee-1019d780545c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reports: 8 example: /home/dataset-assist-0/annual-reports_output/NVIDIA-2018-Annual-Report_short_report.txt\n",
      "You are an expert buy-side analyst and research editor.\n",
      "Evaluate the given equity research summary (the \"Report\") for professional readers.\n",
      "\n",
      "Score each dimension from 1 to 5:\n",
      "- Professionalism: finance terminology, tone, concision, no fluff.\n",
      "- Structure: clear sections, logical flow, scannable bullets where helpful.\n",
      "- Clarity: unambiguous statements, avoids repetition.\n",
      "- Actionability: highlights \n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json, re, time\n",
    "import pandas as pd\n",
    "\n",
    "REPORT_DIR = Path(\"/home/dataset-assist-0/annual-reports_output\")\n",
    "OUT_DIR = Path(\"/home/dataset-assist-0/eval_outputs_geval\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 断点续跑：已存在就跳过\n",
    "OUT_JSONL = OUT_DIR / \"geval_llama2_13b.jsonl\"\n",
    "OUT_CSV   = OUT_DIR / \"geval_llama2_13b.csv\"\n",
    "\n",
    "report_files = sorted(REPORT_DIR.glob(\"*_short_report*.txt\"))\n",
    "print(\"reports:\", len(report_files), \"example:\", report_files[0] if report_files else None)\n",
    "\n",
    "def load_text(p: Path, max_chars=20000):\n",
    "    t = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    t = re.sub(r\"\\n{3,}\", \"\\n\\n\", t).strip()\n",
    "    return t[:max_chars]\n",
    "\n",
    "# 你可以按需改评分维度（G-Eval）\n",
    "RUBRIC = \"\"\"You are an expert buy-side analyst and research editor.\n",
    "Evaluate the given equity research summary (the \"Report\") for professional readers.\n",
    "\n",
    "Score each dimension from 1 to 5:\n",
    "- Professionalism: finance terminology, tone, concision, no fluff.\n",
    "- Structure: clear sections, logical flow, scannable bullets where helpful.\n",
    "- Clarity: unambiguous statements, avoids repetition.\n",
    "- Actionability: highlights key drivers, risks, and implications.\n",
    "- Overall: holistic quality.\n",
    "\n",
    "Return ONLY valid JSON with keys:\n",
    "professionalism, structure, clarity, actionability, overall, rationale (1 short paragraph).\n",
    "\"\"\"\n",
    "\n",
    "def build_prompt(report_text: str) -> str:\n",
    "    return f\"\"\"{RUBRIC}\n",
    "\n",
    "Report:\n",
    "\\\"\\\"\\\"\\n{report_text}\\n\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "print(build_prompt(\"Example report...\")[:400])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "504a70e0-1433-4c00-ae5c-792944066841",
   "metadata": {},
   "source": [
    "在 Notebook 里启动 vLLM OpenAI Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f6bf090-44b7-41e0-8a02-ae5f73963847",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "vLLM failed to start.\nLast logs:\n\u001b[0;36m(APIServer pid=241583)\u001b[0;0m INFO 12-29 20:02:24 [api_server.py:1351] vLLM API server version 0.13.0\n\u001b[0;36m(APIServer pid=241583)\u001b[0;0m INFO 12-29 20:02:24 [utils.py:253] non-default args: {'host': '127.0.0.1', 'model': '/home/dataset-assist-0/data/models/llama2-13b', 'max_model_len': 4096, 'tensor_parallel_size': 2}\n\u001b[0;36m(APIServer pid=241583)\u001b[0;0m INFO 12-29 20:02:24 [model.py:514] Resolved architecture: LlamaForCausalLM\n\u001b[0;36m(APIServer pid=241583)\u001b[0;0m INFO 12-29 20:02:24 [model.py:1661] Using max model len 4096\n\u001b[0;36m(APIServer pid=241583)\u001b[0;0m INFO 12-29 20:02:25 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=2048.\n\u001b[0;36m(EngineCore_DP0 pid=241726)\u001b[0;0m INFO 12-29 20:02:31 [core.py:93] Initializing a V1 LLM engine (v0.13.0) with config: model='/home/dataset-assist-0/data/models/llama2-13b', speculative_config=None, tokenizer='/home/dataset-assist-0/data/models/llama2-13b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/home/dataset-assist-0/data/models/llama2-13b, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}\n\u001b[0;36m(EngineCore_DP0 pid=241726)\u001b[0;0m WARNING 12-29 20:02:31 [multiproc_executor.py:882] Reducing Torch parallelism from 56 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\nINFO 12-29 20:02:36 [parallel_state.py:1203] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:35703 backend=nccl\nERROR 12-29 20:02:37 [multiproc_executor.py:751] WorkerProc failed to start.\nERROR 12-29 20:02:37 [multiproc_executor.py:751] Traceback (most recent call last):\nERROR 12-29 20:02:37 [multiproc_executor.py:751]   File \"/opt/conda/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py\", line 722, in worker_main\nERROR 12-29 20:02:37 [multiproc_executor.py:751]     worker = WorkerProc(*args, **kwargs)\nERROR 12-29 20:02:37 [multiproc_executor.py:751]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 12-29 20:02:37 [multiproc_executor.py:751]   File \"/opt/conda/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py\", line 553, in __init__\nERROR 12-29 20:02:37 [multiproc_executor.py:751]     self.worker.init_device()\nERROR 12-29 20:02:37 [multiproc_executor.py:751]   File \"/opt/conda/lib/python3.11/site-packages/vllm/v1/worker/worker_base.py\", line 326, in init_device\nERROR 12-29 20:02:37 [multiproc_executor.py:751]     self.worker.init_device()  # type: ignore\nERROR 12-29 20:02:37 [multiproc_executor.py:751]     ^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 12-29 20:02:37 [multiproc_executor.py:751]   File \"/opt/conda/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 216, in init_device\nERROR 12-29 20:02:37 [multiproc_executor.py:751]     current_platform.set_device(self.device)\nERROR 12-29 20:02:37 [multiproc_executor.py:751]   File \"/opt/conda/lib/python3.11/site-packages/vllm/platforms/cuda.py\", line 123, in set_device\nERROR 12-29 20:02:37 [multiproc_executor.py:751]     torch.cuda.set_device(device)\nERROR 12-29 20:02:37 [multiproc_executor.py:751]   File \"/opt/conda/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 567, in set_device\nERROR 12-29 20:02:37 [multiproc_executor.py:751]     torch._C._cuda_setDevice(device)\nERROR 12-29 20:02:37 [multiproc_executor.py:751] torch.AcceleratorError: CUDA error: out of memory\nERROR 12-29 20:02:37 [multiproc_executor.py:751] Search for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nERROR 12-29 20:02:37 [multiproc_executor.py:751] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nERROR 12-29 20:02:37 [multiproc_executor.py:751] For debugging consider passing CUDA_LAUNCH_BLOCKING=1\nERROR 12-29 20:02:37 [multiproc_executor.py:751] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\nERROR 12-29 20:02:37 [multiproc_executor.py:751] \nINFO 12-29 20:02:37 [multiproc_executor.py:709] Parent process exited, terminating worker\nINFO 12-29 20:02:37 [multiproc_executor.py:709] Parent process exited, terminating worker\n[W1229 20:02:37.462299527 TCPStore.cpp:340] [c10d] TCP client failed to connect/validate to host 127.0.0.1:35703 - retrying (try=0, timeout=600000ms, delay=474ms): Interrupted system call\nException raised from delay at /pytorch/torch/csrc/distributed/c10d/socket.cpp:115 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7fc90a22bb80 in /opt/conda/lib/python3.11/site-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x5ffd531 (0x7fc94c633531 in /opt/conda/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\nframe #2: <unknown function> + 0x14a173a (0x7fc947ad773a in /opt/conda/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\nframe #3: <unknown function> + 0x60791eb (0x7fc94c6af1eb in /opt/conda/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\nframe #4: <unknown function> + 0x6079584 (0x7fc94c6af584 in /opt/conda/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\nframe #5: <unknown function> + 0x5ff5e03 (0x7fc94c62be03 in /opt/conda/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\nframe #6: c10d::TCPStore::TCPStore(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, c10d::TCPStoreOptions const&) + 0x41d (0x7fc94c63282d in /opt/conda/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\nframe #7: <unknown function> + 0xd71465 (0x7fc95bd4e465 in /opt/conda/lib/python3.11/site-packages/torch/lib/libtorch_python.so)\nframe #8: <unknown function> + 0xdadda6 (0x7fc95bd8ada6 in /opt/conda/lib/python3.11/site-packages/torch/lib/libtorch_python.so)\nframe #9: <unknown function> + 0x3cc7bd (0x7fc95b3a97bd in /opt/conda/lib/python3.11/site-packages/torch/lib/libtorch_python.so)\nframe #10: /opt/conda/bin/python() [0x528527]\nframe #11: _PyObject_MakeTpCall + 0x254 (0x504f04 in /opt/conda/bin/python)\nframe #12: /opt/conda/bin/python() [0x556d63]\nframe #13: _PyObject_Call + 0x93 (0x542503 in /opt/conda/bin/python)\nframe #14: /opt/conda/bin/python() [0x53fe52]\nframe #15: /opt/conda/bin/python() [0x50542c]\nframe #16: <unknown function> + 0x3caeab (0x7fc95b3a7eab in /opt/conda/lib/python3.11/site-packages/torch/lib/libtorch_python.so)\nframe #17: _PyObject_MakeTpCall + 0x254 (0x504f04 in /opt/conda/bin/python)\nframe #18: _PyEval_EvalFrameDefault + 0x753 (0x5111d3 in /opt/conda/bin/python)\nframe #19: /opt/conda/bin/python() [0x59dcd7]\nframe #20: /opt/conda/bin/python() [0x52ea0b]\nframe #21: PyObject_Vectorcall + 0x31 (0x51e131 in /opt/conda/bin/python)\nframe #22: _PyEval_EvalFrameDefault + 0x753 (0x5111d3 in /opt/conda/bin/python)\nframe #23: _PyFunction_Vectorcall + 0x173 (0x5380f3 in /opt/conda/bin/python)\nframe #24: PyObject_Call + 0xa2 (0x542292 in /opt/conda/bin/python)\nframe #25: _PyEval_EvalFrameDefault + 0x44bd (0x514f3d in /opt/conda/bin/python)\nframe #26: _PyFunction_Vectorcall + 0x173 (0x5380f3 in /opt/conda/bin/python)\nframe #27: PyObject_Call + 0xa2 (0x542292 in /opt/conda/bin/python)\nframe #28: _PyEval_EvalFrameDefault + 0x44bd (0x514f3d in /opt/conda/bin/python)\nframe #29: _PyFunction_Vectorcall + 0x173 (0x5380f3 in /opt/conda/bin/python)\nframe #30: /opt/conda/bin/python() [0x53fba2]\nframe #31: /opt/conda/bin/python() [0x50542c]\nframe #32: PyObject_Call + 0x20b (0x5423fb in /opt/conda/bin/python)\nframe #33: _PyEval_EvalFrameDefault + 0x44bd (0x514f3d in /opt/conda/bin/python)\nframe #34: _PyFunction_Vectorcall + 0x173 (0x5380f3 in /opt/conda/bin/python)\nframe #35: PyObject_Call + 0xa2 (0x542292 in /opt/conda/bin/python)\nframe #36: _PyEval_EvalFrameDefault + 0x44bd (0x514f3d in /opt/conda/bin/python)\nframe #37: /opt/conda/bin/python() [0x5caeae]\nframe #38: PyEval_EvalCode + 0x9f (0x5ca4ef in /opt/conda/bin/python)\nframe #39: /opt/conda/bin/python() [0x5ec747]\nframe #40: /opt/conda/bin/python() [0x5e8af0]\nframe #41: PyRun_StringFlags + 0x5f (0x5db1bf in /opt/conda/bin/python)\nframe #42: PyRun_SimpleStringFlags + 0x3b (0x5db06b in /opt/conda/bin/python)\nframe #43: Py_RunMain + 0x3e8 (0x5f6cd8 in /opt/conda/bin/python)\nframe #44: Py_BytesMain + 0x39 (0x5b9a79 in /opt/conda/bin/python)\nframe #45: __libc_start_main + 0xf3 (0x7fc96a033083 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvLLM failed to start.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLast logs:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(lines))\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvLLM is up:\u001b[39m\u001b[38;5;124m\"\u001b[39m, requests\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mVLLM_BASE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/models\u001b[39m\u001b[38;5;124m\"\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39mjson())\n",
      "\u001b[0;31mRuntimeError\u001b[0m: vLLM failed to start.\nLast logs:\n\u001b[0;36m(APIServer pid=241583)\u001b[0;0m INFO 12-29 20:02:24 [api_server.py:1351] vLLM API server version 0.13.0\n\u001b[0;36m(APIServer pid=241583)\u001b[0;0m INFO 12-29 20:02:24 [utils.py:253] non-default args: {'host': '127.0.0.1', 'model': '/home/dataset-assist-0/data/models/llama2-13b', 'max_model_len': 4096, 'tensor_parallel_size': 2}\n\u001b[0;36m(APIServer pid=241583)\u001b[0;0m INFO 12-29 20:02:24 [model.py:514] Resolved architecture: LlamaForCausalLM\n\u001b[0;36m(APIServer pid=241583)\u001b[0;0m INFO 12-29 20:02:24 [model.py:1661] Using max model len 4096\n\u001b[0;36m(APIServer pid=241583)\u001b[0;0m INFO 12-29 20:02:25 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=2048.\n\u001b[0;36m(EngineCore_DP0 pid=241726)\u001b[0;0m INFO 12-29 20:02:31 [core.py:93] Initializing a V1 LLM engine (v0.13.0) with config: model='/home/dataset-assist-0/data/models/llama2-13b', speculative_config=None, tokenizer='/home/dataset-assist-0/data/models/llama2-13b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/home/dataset-assist-0/data/models/llama2-13b, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}\n\u001b[0;36m(EngineCore_DP0 pid=241726)\u001b[0;0m WARNING 12-29 20:02:31 [multiproc_executor.py:882] Reducing Torch parallelism from 56 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\nINFO 12-29 20:02:36 [parallel_state.py:1203] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:35703 backend=nccl\nERROR 12-29 20:02:37 [multiproc_executor.py:751] WorkerProc failed to start.\nERROR 12-29 20:02:37 [multiproc_executor.py:751] Traceback (most recent call last):\nERROR 12-29 20:02:37 [multiproc_executor.py:751]   File \"/opt/conda/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py\", line 722, in worker_main\nERROR 12-29 20:02:37 [multiproc_executor.py:751]     worker = WorkerProc(*args, **kwargs)\nERROR 12-29 20:02:37 [multiproc_executor.py:751]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 12-29 20:02:37 [multiproc_executor.py:751]   File \"/opt/conda/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py\", line 553, in __init__\nERROR 12-29 20:02:37 [multiproc_executor.py:751]     self.worker.init_device()\nERROR 12-29 20:02:37 [multiproc_executor.py:751]   File \"/opt/conda/lib/python3.11/site-packages/vllm/v1/worker/worker_base.py\", line 326, in init_device\nERROR 12-29 20:02:37 [multiproc_executor.py:751]     self.worker.init_device()  # type: ignore\nERROR 12-29 20:02:37 [multiproc_executor.py:751]     ^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 12-29 20:02:37 [multiproc_executor.py:751]   File \"/opt/conda/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 216, in init_device\nERROR 12-29 20:02:37 [multiproc_executor.py:751]     current_platform.set_device(self.device)\nERROR 12-29 20:02:37 [multiproc_executor.py:751]   File \"/opt/conda/lib/python3.11/site-packages/vllm/platforms/cuda.py\", line 123, in set_device\nERROR 12-29 20:02:37 [multiproc_executor.py:751]     torch.cuda.set_device(device)\nERROR 12-29 20:02:37 [multiproc_executor.py:751]   File \"/opt/conda/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 567, in set_device\nERROR 12-29 20:02:37 [multiproc_executor.py:751]     torch._C._cuda_setDevice(device)\nERROR 12-29 20:02:37 [multiproc_executor.py:751] torch.AcceleratorError: CUDA error: out of memory\nERROR 12-29 20:02:37 [multiproc_executor.py:751] Search for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nERROR 12-29 20:02:37 [multiproc_executor.py:751] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nERROR 12-29 20:02:37 [multiproc_executor.py:751] For debugging consider passing CUDA_LAUNCH_BLOCKING=1\nERROR 12-29 20:02:37 [multiproc_executor.py:751] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\nERROR 12-29 20:02:37 [multiproc_executor.py:751] \nINFO 12-29 20:02:37 [multiproc_executor.py:709] Parent process exited, terminating worker\nINFO 12-29 20:02:37 [multiproc_executor.py:709] Parent process exited, terminating worker\n[W1229 20:02:37.462299527 TCPStore.cpp:340] [c10d] TCP client failed to connect/validate to host 127.0.0.1:35703 - retrying (try=0, timeout=600000ms, delay=474ms): Interrupted system call\nException raised from delay at /pytorch/torch/csrc/distributed/c10d/socket.cpp:115 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7fc90a22bb80 in /opt/conda/lib/python3.11/site-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x5ffd531 (0x7fc94c633531 in /opt/conda/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\nframe #2: <unknown function> + 0x14a173a (0x7fc947ad773a in /opt/conda/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\nframe #3: <unknown function> + 0x60791eb (0x7fc94c6af1eb in /opt/conda/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\nframe #4: <unknown function> + 0x6079584 (0x7fc94c6af584 in /opt/conda/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\nframe #5: <unknown function> + 0x5ff5e03 (0x7fc94c62be03 in /opt/conda/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\nframe #6: c10d::TCPStore::TCPStore(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, c10d::TCPStoreOptions const&) + 0x41d (0x7fc94c63282d in /opt/conda/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)\nframe #7: <unknown function> + 0xd71465 (0x7fc95bd4e465 in /opt/conda/lib/python3.11/site-packages/torch/lib/libtorch_python.so)\nframe #8: <unknown function> + 0xdadda6 (0x7fc95bd8ada6 in /opt/conda/lib/python3.11/site-packages/torch/lib/libtorch_python.so)\nframe #9: <unknown function> + 0x3cc7bd (0x7fc95b3a97bd in /opt/conda/lib/python3.11/site-packages/torch/lib/libtorch_python.so)\nframe #10: /opt/conda/bin/python() [0x528527]\nframe #11: _PyObject_MakeTpCall + 0x254 (0x504f04 in /opt/conda/bin/python)\nframe #12: /opt/conda/bin/python() [0x556d63]\nframe #13: _PyObject_Call + 0x93 (0x542503 in /opt/conda/bin/python)\nframe #14: /opt/conda/bin/python() [0x53fe52]\nframe #15: /opt/conda/bin/python() [0x50542c]\nframe #16: <unknown function> + 0x3caeab (0x7fc95b3a7eab in /opt/conda/lib/python3.11/site-packages/torch/lib/libtorch_python.so)\nframe #17: _PyObject_MakeTpCall + 0x254 (0x504f04 in /opt/conda/bin/python)\nframe #18: _PyEval_EvalFrameDefault + 0x753 (0x5111d3 in /opt/conda/bin/python)\nframe #19: /opt/conda/bin/python() [0x59dcd7]\nframe #20: /opt/conda/bin/python() [0x52ea0b]\nframe #21: PyObject_Vectorcall + 0x31 (0x51e131 in /opt/conda/bin/python)\nframe #22: _PyEval_EvalFrameDefault + 0x753 (0x5111d3 in /opt/conda/bin/python)\nframe #23: _PyFunction_Vectorcall + 0x173 (0x5380f3 in /opt/conda/bin/python)\nframe #24: PyObject_Call + 0xa2 (0x542292 in /opt/conda/bin/python)\nframe #25: _PyEval_EvalFrameDefault + 0x44bd (0x514f3d in /opt/conda/bin/python)\nframe #26: _PyFunction_Vectorcall + 0x173 (0x5380f3 in /opt/conda/bin/python)\nframe #27: PyObject_Call + 0xa2 (0x542292 in /opt/conda/bin/python)\nframe #28: _PyEval_EvalFrameDefault + 0x44bd (0x514f3d in /opt/conda/bin/python)\nframe #29: _PyFunction_Vectorcall + 0x173 (0x5380f3 in /opt/conda/bin/python)\nframe #30: /opt/conda/bin/python() [0x53fba2]\nframe #31: /opt/conda/bin/python() [0x50542c]\nframe #32: PyObject_Call + 0x20b (0x5423fb in /opt/conda/bin/python)\nframe #33: _PyEval_EvalFrameDefault + 0x44bd (0x514f3d in /opt/conda/bin/python)\nframe #34: _PyFunction_Vectorcall + 0x173 (0x5380f3 in /opt/conda/bin/python)\nframe #35: PyObject_Call + 0xa2 (0x542292 in /opt/conda/bin/python)\nframe #36: _PyEval_EvalFrameDefault + 0x44bd (0x514f3d in /opt/conda/bin/python)\nframe #37: /opt/conda/bin/python() [0x5caeae]\nframe #38: PyEval_EvalCode + 0x9f (0x5ca4ef in /opt/conda/bin/python)\nframe #39: /opt/conda/bin/python() [0x5ec747]\nframe #40: /opt/conda/bin/python() [0x5e8af0]\nframe #41: PyRun_StringFlags + 0x5f (0x5db1bf in /opt/conda/bin/python)\nframe #42: PyRun_SimpleStringFlags + 0x3b (0x5db06b in /opt/conda/bin/python)\nframe #43: Py_RunMain + 0x3e8 (0x5f6cd8 in /opt/conda/bin/python)\nframe #44: Py_BytesMain + 0x39 (0x5b9a79 in /opt/conda/bin/python)\nframe #45: __libc_start_main + 0xf3 (0x7fc96a033083 in /usr/lib/x86_64-linux-gnu/libc.so.6)\n"
     ]
    }
   ],
   "source": [
    "import os, sys, subprocess, time, requests, textwrap, signal\n",
    "\n",
    "MODEL_DIR = \"/home/dataset-assist-0/data/models/llama2-13b\"\n",
    "VLLM_BASE = \"http://127.0.0.1:8000/v1\"\n",
    "\n",
    "env = os.environ.copy()\n",
    "env[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "env[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "\n",
    "cmd = [\n",
    "    sys.executable, \"-m\", \"vllm.entrypoints.openai.api_server\",\n",
    "    \"--model\", MODEL_DIR,\n",
    "    \"--host\", \"127.0.0.1\", \"--port\", \"8000\",\n",
    "    \"--dtype\", \"auto\",\n",
    "    \"--tensor-parallel-size\", \"2\",\n",
    "    \"--max-model-len\", \"4096\",\n",
    "    \"--gpu-memory-utilization\", \"0.90\",\n",
    "]\n",
    "\n",
    "proc = subprocess.Popen(cmd, env=env, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "# 等待服务起来\n",
    "ok = False\n",
    "for _ in range(120):\n",
    "    try:\n",
    "        r = requests.get(f\"{VLLM_BASE}/models\", timeout=1.5)\n",
    "        if r.status_code == 200:\n",
    "            ok = True\n",
    "            break\n",
    "    except Exception:\n",
    "        pass\n",
    "    time.sleep(1)\n",
    "\n",
    "if not ok:\n",
    "    # 打印最近日志，方便定位\n",
    "    lines = []\n",
    "    try:\n",
    "        for _ in range(80):\n",
    "            lines.append(proc.stdout.readline())\n",
    "    except Exception:\n",
    "        pass\n",
    "    raise RuntimeError(\"vLLM failed to start.\\nLast logs:\\n\" + \"\".join(lines))\n",
    "\n",
    "print(\"vLLM is up:\", requests.get(f\"{VLLM_BASE}/models\", timeout=10).json())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "71c794b7-0a7d-4125-9eed-782e595f7a67",
   "metadata": {},
   "source": [
    "批量请求 vLLM /v1/chat/completions，输出 jsonl + csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93289394-3f28-464b-998c-d552e93e025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "VLLM_BASE = \"http://127.0.0.1:8000/v1\"\n",
    "MODEL_ID = \"llama2-13b-judge\"\n",
    "\n",
    "def chat_once(prompt: str, temperature=0.0, max_tokens=400):\n",
    "    payload = {\n",
    "        \"model\": MODEL_ID,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a strict evaluator. Output JSON only.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "    }\n",
    "    r = requests.post(f\"{VLLM_BASE}/chat/completions\", json=payload, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    return r.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "def extract_json(s: str):\n",
    "    m = re.search(r\"\\{.*\\}\", s, flags=re.S)\n",
    "    if not m:\n",
    "        raise ValueError(\"No JSON found\")\n",
    "    return json.loads(m.group(0))\n",
    "\n",
    "# 断点续跑：读已有 jsonl\n",
    "done = set()\n",
    "rows = []\n",
    "if OUT_JSONL.exists():\n",
    "    for line in OUT_JSONL.read_text(encoding=\"utf-8\").splitlines():\n",
    "        if not line.strip(): \n",
    "            continue\n",
    "        obj = json.loads(line)\n",
    "        done.add(obj[\"file\"])\n",
    "        rows.append(obj)\n",
    "\n",
    "print(\"already done:\", len(done))\n",
    "\n",
    "with OUT_JSONL.open(\"a\", encoding=\"utf-8\") as f:\n",
    "    for rp in tqdm(report_files):\n",
    "        if rp.name in done:\n",
    "            continue\n",
    "\n",
    "        report_text = load_text(rp, max_chars=20000)\n",
    "        prompt = build_prompt(report_text)\n",
    "\n",
    "        t0 = time.time()\n",
    "        try:\n",
    "            raw = chat_once(prompt, temperature=0.0, max_tokens=400)\n",
    "            j = extract_json(raw)\n",
    "            rec = {\n",
    "                \"file\": rp.name,\n",
    "                \"professionalism\": j.get(\"professionalism\"),\n",
    "                \"structure\": j.get(\"structure\"),\n",
    "                \"clarity\": j.get(\"clarity\"),\n",
    "                \"actionability\": j.get(\"actionability\"),\n",
    "                \"overall\": j.get(\"overall\"),\n",
    "                \"rationale\": j.get(\"rationale\"),\n",
    "                \"latency_s\": round(time.time() - t0, 3),\n",
    "            }\n",
    "        except Exception as e:\n",
    "            rec = {\n",
    "                \"file\": rp.name,\n",
    "                \"error\": str(e),\n",
    "                \"latency_s\": round(time.time() - t0, 3),\n",
    "            }\n",
    "\n",
    "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "        f.flush()\n",
    "        rows.append(rec)\n",
    "        done.add(rp.name)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "df.head(), df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a865ff77-c77f-444e-bf0c-349944180411",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
